llama-index
llama-index-llms-huggingface
llama-index-program-guidance

# llama-cpp-python==0.2.26 #for {Guidance} version 0.2.60 results in inconsistent output
# for cuda support: CUDACXX=/usr/local/cuda-12/bin/nvcc CMAKE_ARGS="-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all-major" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.26 --no-cache-dir --force-reinstall --upgrade
llama-cpp-python==0.2.61

llama-index-llms-llama-cpp
llama-index-extractors-entity
