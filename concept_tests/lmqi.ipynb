{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMQI with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTENT:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "CONCLUSIONS:\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lmql.ai/docs/latest/lib/integrations/llama_index.html - outdated example with regard to query/query_engine \\\n",
    "https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: to runt the notebook\n",
    "1. Remove 'local:' from llm = lmql.model(\"local:llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\"), \n",
    "1. start a service in terminal with: lmql serve-model llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf --verbose True --n_gpu_layers 20 --n_ctx 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "from llama_index.core import GPTVectorStoreIndex, VectorStoreIndex, SimpleDirectoryReader, ServiceContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama.cpp endpoint: https://lmql.ai/docs/models/llama.cpp.html#running-without-a-model-server\n",
    "# tokenizer.model from https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main\n",
    "\n",
    "llm = lmql.model(\"llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\", n_gpu_layers=10, n_ctx=0, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all documents from assigned folder\n",
    "documents = SimpleDirectoryReader(\"/home/dorota/LLM-diploma-project/00_concept_tests/data/cancer_article\").load_data() # -> list of 1 doc/page in article with metadata and tags (documents[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables to create vector embeddings for text nodes\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2').encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents) #[0:1]\n",
    "\n",
    "Settings.llm = None # =None to enable correct setting in query_engine\n",
    "query_engine = index.as_query_engine(streaming=True, llm=None) # llm=None sets llm to Settings.llm thus defined as None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_top_k = 2\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "async def index_query(question: str):\n",
    "    '''lmql\n",
    "    \"You are a QA bot that helps users answer questions.\\n\"\n",
    "    \n",
    "    # ask the question\n",
    "    \"Question: {question}\\n\"\n",
    "\n",
    "    # look up and insert relevant information into the context\n",
    "    response = query_engine.query(question)\n",
    "    information = \"\\n\\n\".join([s.node.get_text() for s in response.source_nodes])\n",
    "    print(information)\n",
    "    \"\\nRelevant Information: {information}\\n\"\n",
    "    \n",
    "    # generate a response\n",
    "    \"Your response based on relevant information:[RESPONSE]\"\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await index_query(\"What is the title of the article?\", \n",
    "                   output_writer=lmql.stream(variable=\"RESPONSE\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
