{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMQI with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTENT:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "CONCLUSIONS:\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStoreIndex with documents vs nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lmql.ai/docs/latest/lib/integrations/llama_index.html - outdated example with regard to query/query_engine \\\n",
    "https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: to runt the notebook\n",
    "1. Remove 'local:' from llm = lmql.model(\"local:llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\"), \n",
    "1. start a service in terminal with: lmql serve-model llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf --verbose True --n_gpu_layers 20 --n_ctx 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dorota/LLM-diploma-project/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lmql\n",
    "from llama_index.core import GPTVectorStoreIndex, VectorStoreIndex, SimpleDirectoryReader, ServiceContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama.cpp endpoint: https://lmql.ai/docs/models/llama.cpp.html#running-without-a-model-server\n",
    "# tokenizer.model from https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main\n",
    "\n",
    "llm = lmql.model(\"llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\", n_gpu_layers=10, n_ctx=0, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all documents from assigned folder\n",
    "documents = SimpleDirectoryReader(input_files=[\"/home/dorota/LLM-diploma-project/00_concept_tests/data/40001_2023_Article_1364.pdf\"]).load_data() # -> list of Document objects with 1 doc/page in article with metadata and tags (documents[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables to create vector embeddings for text nodes\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2').encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 21/21 [00:00<00:00, 251.36it/s]\n",
      "Generating embeddings: 100%|██████████| 47/47 [00:00<00:00, 191.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True) #[0:1] # index = VectorStoreIndex(nodes)\n",
    "\n",
    "Settings.llm = None # =None to enable correct setting in query_engine\n",
    "query_engine = index.as_query_engine(streaming=True, llm=None) # llm=None sets llm to Settings.llm thus defined as None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is the main topic of the article?\"\n",
    "# response = query_engine.query(question)\n",
    "# response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.source_nodes[0].node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with Semantic nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "index= VectorStoreIndex(nodes)\n",
    "\n",
    "Settings.llm = None\n",
    "query_engine = index.as_query_engine(streaming=True, llm=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_top_k = 2\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "async def index_query(question: str):\n",
    "    '''lmql\n",
    "    \"You are a QA bot that helps users answer questions.\\n\"\n",
    "    \n",
    "    # ask the question\n",
    "    \"Question: {question}\\n\"\n",
    "\n",
    "    # look up and insert relevant information into the context\n",
    "    response = query_engine.query(question)\n",
    "    for s in response.source_nodes:\n",
    "        print(s.node.get_text())\n",
    "        print('----------------------------------------------------------')\n",
    "    information = \"\\n\\n\".join([s.node.get_text() for s in response.source_nodes])\n",
    "    \"\\nRelevant Information: {information}\\n\"\n",
    "    \n",
    "    # generate a response\n",
    "    \"Your response based on relevant information:[RESPONSE]\" where len(RESPONSE) < 200 and STOPS_AT(RESPONSE, \".\")\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 14 of 21 Xu et al. European Journal of Medical Research          (2023) 28:461 \n",
      "and new knowledge that emerged. \n",
      "----------------------------------------------------------\n",
      "Employing a segmentation process, topics exhibit -\n",
      "ing akin clusters were deftly allocated to cohesive areas, \n",
      "thereby engendering a heightened sense of organization \n",
      "and a more comprehensive grasp of the underlying data \n",
      "(Fig.  8a). In this analysis, a keyword co-occurrence analy -\n",
      "sis was conducted to identify the most frequently appear -\n",
      "ing terms. The analysis included five keywords: “breast \n",
      "cancer” with 1339 occurrences, “expression” with 831 \n",
      "occurrences, “cancer” with 407 occurrences, “protein” \n",
      "with 358 occurrences, and “translation” with 350 occur -\n",
      "rences. These results suggest that the analysis primarily \n",
      "focused on the relationship between breast cancer and \n",
      "protein synthesis, including gene expression, translation, \n",
      "and apoptosis. The aim of this analysis was to identify the \n",
      "most frequent keywords related to breast cancer-related \n",
      "Fig. 7 Co-citation network map of authors of breast cancer-related protein synthesis for the period 2003 to 2022\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main finding of the study is that the analysis primarily focused on the relationship between breast cancer and protein synthesis, including gene expression, translation, and apoptosis."
     ]
    }
   ],
   "source": [
    "result = await index_query(\"What is the main finding?\", \n",
    "                   output_writer=lmql.stream(variable=\"RESPONSE\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMQL output in a dataclass for strucutred output\n",
    "https://lmql.ai/blog/ \\\n",
    "https://www.timlrx.com/blog/generating-structured-output-from-llms#lmql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe(recipe_name='Spaghetti Bolognese', servings=4, ingredient1=Ingredient(name='Spaghetti', weight_in_grams=450), ingredient2=Ingredient(name='Olive Oil', weight_in_grams=2), ingredient3=Ingredient(name='Onion', weight_in_grams=150), ingredient4=Ingredient(name='Garlic', weight_in_grams=2), ingredient5=Ingredient(name='Carrots', weight_in_grams=150), ingredient6=Ingredient(name='Celery', weight_in_grams=50), ingredient7=Ingredient(name='Ground Beef', weight_in_grams=450), ingredient8=Ingredient(name='Tomato Sauce', weight_in_grams=800))\n"
     ]
    }
   ],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Ingredient:\n",
    "    name: str\n",
    "    weight_in_grams: int\n",
    "\n",
    "@dataclass\n",
    "class Recipe:\n",
    "    recipe_name: str\n",
    "    servings: int\n",
    "    ingredient1: Ingredient\n",
    "    ingredient2: Ingredient\n",
    "    ingredient3: Ingredient\n",
    "    ingredient4: Ingredient\n",
    "    ingredient5: Ingredient\n",
    "    ingredient6: Ingredient\n",
    "    ingredient7: Ingredient\n",
    "    ingredient8: Ingredient\n",
    "    # list not supported...\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "async def spaghetti():\n",
    "    '''lmql\n",
    "    \"Spaghetti bolognese recipe for a family of 4.\"\n",
    "    \"[RECIPE_DATA]\\\\n\" where type(RECIPE_DATA) is Recipe\n",
    "    return RECIPE_DATA\n",
    "    '''\n",
    "\n",
    "result = await spaghetti()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recipe(recipe_name='Spaghetti Bolognese', servings=4, ingredient1=Ingredient(name='Spaghetti', weight_in_grams=450), ingredient2=Ingredient(name='Olive Oil', weight_in_grams=2), ingredient3=Ingredient(name='Onion', weight_in_grams=150), ingredient4=Ingredient(name='Garlic', weight_in_grams=2), ingredient5=Ingredient(name='Carrots', weight_in_grams=150), ingredient6=Ingredient(name='Celery', weight_in_grams=50), ingredient7=Ingredient(name='Ground Beef', weight_in_grams=450), ingredient8=Ingredient(name='Tomato Sauce', weight_in_grams=800))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_name': 'Spaghetti Bolognese',\n",
       " 'servings': 4,\n",
       " 'ingredient1': Ingredient(name='Spaghetti', weight_in_grams=450),\n",
       " 'ingredient2': Ingredient(name='Olive Oil', weight_in_grams=2),\n",
       " 'ingredient3': Ingredient(name='Onion', weight_in_grams=150),\n",
       " 'ingredient4': Ingredient(name='Garlic', weight_in_grams=2),\n",
       " 'ingredient5': Ingredient(name='Carrots', weight_in_grams=150),\n",
       " 'ingredient6': Ingredient(name='Celery', weight_in_grams=50),\n",
       " 'ingredient7': Ingredient(name='Ground Beef', weight_in_grams=450),\n",
       " 'ingredient8': Ingredient(name='Tomato Sauce', weight_in_grams=800)}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spaghetti Bolognese'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.recipe_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of thought example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The day one week ago was the 5th of June.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "def chain_of_thought(question):\n",
    "    '''lmql\n",
    "    # Q&A prompt template\n",
    "    \"Q: {question}\\n\"\n",
    "    \"A: Let's think step by step.\\n\"\n",
    "    \"[REASONING]\"\n",
    "    \"Thus, the answer is:[ANSWER].\" where STOPS_AT(ANSWER, \".\")\n",
    "\n",
    "    # return just the ANSWER to the caller\n",
    "    return ANSWER.strip()\n",
    "    '''\n",
    "\n",
    "result = chain_of_thought('Today is the 12th of June, what day was it 1 week ago?')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data with SimpleDirectoryReader\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/\n",
    "\n",
    "more readers availble at https://llamahub.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You can specify a function that will read each file and extract metadata that gets attached to the resulting Document\n",
    "# def get_meta(file_path):\n",
    "#     return {\"foo\": \"bar\", \"file_path\": file_path}\n",
    "\n",
    "\n",
    "# SimpleDirectoryReader(input_files=[\"/home/dorota/LLM-diploma-project/00_concept_tests/data/40001_2023_Article_1364.pdf\"], file_metadata=get_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # additional possibilities with SimpleDirectoryReader\n",
    "# documents = SimpleDirectoryReader(input_dir=\"/home/dorota/LLM-diploma-project/00_concept_tests/data\", recursive=True).load_data(num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create nodes with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SentenceSplitter\n",
    "The SentenceSplitter attempts to split text in chunks while respecting the boundaries of sentences. \\\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can be defined globaly\n",
    "# Settings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "# # an be dafound per-index through transformations\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SentenceWindowNodeParser\n",
    "Splits all documents into individual sentences. The resulting nodes also contain the surrounding \"window\" of sentences around each node in the metadata.\\\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=2,  # how many sentences on either side to capture\n",
    "    window_metadata_key=\"window\", # the metadata key that holds the window of surrounding sentences\n",
    "    original_text_metadata_key=\"original_sentence\", # the metadata key that holds the original sentence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SemanticSplitterNodeParser\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. HierarchicalNodeParser\n",
    "Input is chunked into several hierarchies of chunk sizes, with each node containing a reference to it's parent node. https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/ \\\n",
    "When combined with the AutoMergingRetriever, this enables us to automatically replace retrieved nodes with their parents when a majority of children are retrieved. https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever/ (conclusion in tutorial that output quality similar to non hierarchical approach...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk into parent, child, grandchild (leaf) nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "\n",
    "splitter = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128] # chunk size parent, child, grandchild\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate grandchild nodes from root nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes\n",
    "\n",
    "base_nodes = get_leaf_nodes(nodes)\n",
    "root_nodes = get_root_nodes(nodes)\n",
    "\n",
    "len(base_nodes), len(root_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all nodes into SimpleDocumentStore and only leaf nodes into VectoreStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore) # define storage context (will include vector store by default too)\n",
    "\n",
    "## Load index into vector index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "base_index = VectorStoreIndex(\n",
    "    base_nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=3)\n",
    "retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n",
    "\n",
    "# query_str = (\"What is the title of the article?\")\n",
    "query_str = (\"What is the main topic of the article?\")\n",
    "\n",
    "nodes = retriever.retrieve(query_str)\n",
    "base_nodes = base_retriever.retrieve(query_str)\n",
    "\n",
    "len(nodes), len(base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "import matplotlib\n",
    "\n",
    "for node in base_nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TokenTextSplitter https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: seem to be the same output: nodes.get_content(), nodes.text, nodes.get_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
