{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nodes with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*NOTE: to runt the notebook*\n",
    "1. *Remove 'local:' from llm = lmql.model(\"local:llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\")*\n",
    "1. *start a service in terminal with: lmql serve-model llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf --verbose True --n_gpu_layers 20 --n_ctx 0*\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTENT: Investigation of chunking into nodes with LlamaIndex build in tools using mistral-7b-instruct-Q6 on scientific article and patents. Additional investigation of node creation using different node splitters provided by LlamaIndex.\n",
    "\n",
    "RESULTS AND COMMENTS:\n",
    "NOTE: very limited investigation!\n",
    "* article -> documents 21 (1/page) -> 48 sematic nodes : better results with semantic nodes\n",
    "* patent1 -> documaents 17 (1/page) -> 26 semantic nodes: better results with documents\n",
    "* patent2 -> documents 47 (1/page) -> 84 semantic nodes: better results with documents\n",
    "\n",
    "* semantic nodes for both article and patent typically start with footnote stating the title/number/page for example \"7 EP 2 671 601 A1 8\" or \"WO 2014/076653 PCT/IB2013/060133\" and another half way through the page. Furhter investigation after cleaning the text and larger sample is needed to draw any conclusions.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dorota/LLM-diploma-project/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lmql\n",
    "from llama_index.core import GPTVectorStoreIndex, VectorStoreIndex, SimpleDirectoryReader, ServiceContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama.cpp endpoint: https://lmql.ai/docs/models/llama.cpp.html#running-without-a-model-server\n",
    "# tokenizer.model from https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main\n",
    "\n",
    "llm = lmql.model(\"llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\", n_gpu_layers=10, n_ctx=0, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables to create vector embeddings for text nodes\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2').encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStoreIndex with documents or sematic nodes tested on scientific article and patent\n",
    "SimpleDirectoryReader: https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/ \\\n",
    "more readers availble at https://llamahub.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all documents from assigned folder\n",
    "# documents = SimpleDirectoryReader(input_files=[\"/home/dorota/LLM-diploma-project/00_concept_tests/data/40001_2023_Article_1364.pdf\"]).load_data() \n",
    "# documents = SimpleDirectoryReader(input_files=[\"/home/dorota/LLM-diploma-project/00_concept_tests/data/patents/EP2671601A1.pdf\"]).load_data()\n",
    "documents = SimpleDirectoryReader(input_files=[\"/home/dorota/LLM-diploma-project/00_concept_tests/data/patents/WO2014076653A1.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents)\n",
    "# -> list of Document objects with 1 doc/page with metadata and tags (documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indexing documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True) #[0:1] # index = VectorStoreIndex(nodes)\n",
    "\n",
    "Settings.llm = None # =None to enable correct setting in query_engine\n",
    "query_engine = index.as_query_engine(streaming=True, llm=None) # llm=None sets llm to Settings.llm thus defined as None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indexing Semantic nodes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(buffer_size=1, breakpoint_percentile_threshold=99, embed_model=Settings.embed_model)\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "index= VectorStoreIndex(nodes)\n",
    "Settings.llm = None\n",
    "query_engine = index.as_query_engine(streaming=True, llm=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    print(node.text)\n",
    "    print(\"----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_top_k = 4\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "async def index_query(question: str):\n",
    "    '''lmql\n",
    "    \"You are a QA bot that helps users answer questions.\\n\"\n",
    "    \n",
    "    # ask the question\n",
    "    \"Question: {question}\\n\"\n",
    "\n",
    "    # look up and insert relevant information into the context\n",
    "    response = query_engine.query(question)\n",
    "    for s in response.source_nodes:\n",
    "        print(s.node.get_text())\n",
    "        print('----------------------------------------------------------')\n",
    "    information = \"\\n\\n\".join([s.node.get_text() for s in response.source_nodes])\n",
    "    \"\\nRelevant Information: {information}\\n\"\n",
    "    \n",
    "    # generate a response\n",
    "    \"Your response based on relevant information:[RESPONSE]\" where STOPS_AT(RESPONSE, \".\")\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... extracting info from scientific article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 14 of 21 Xu et al. European Journal of Medical Research          (2023) 28:461 \n",
      "and new knowledge that emerged. \n",
      "----------------------------------------------------------\n",
      "Employing a segmentation process, topics exhibit -\n",
      "ing akin clusters were deftly allocated to cohesive areas, \n",
      "thereby engendering a heightened sense of organization \n",
      "and a more comprehensive grasp of the underlying data \n",
      "(Fig.  8a). In this analysis, a keyword co-occurrence analy -\n",
      "sis was conducted to identify the most frequently appear -\n",
      "ing terms. The analysis included five keywords: “breast \n",
      "cancer” with 1339 occurrences, “expression” with 831 \n",
      "occurrences, “cancer” with 407 occurrences, “protein” \n",
      "with 358 occurrences, and “translation” with 350 occur -\n",
      "rences. These results suggest that the analysis primarily \n",
      "focused on the relationship between breast cancer and \n",
      "protein synthesis, including gene expression, translation, \n",
      "and apoptosis. The aim of this analysis was to identify the \n",
      "most frequent keywords related to breast cancer-related \n",
      "Fig. 7 Co-citation network map of authors of breast cancer-related protein synthesis for the period 2003 to 2022\n",
      "----------------------------------------------------------\n",
      "The main finding of the study is that the analysis primarily focused on the relationship between breast cancer and protein synthesis, including gene expression, translation, and apoptosis."
     ]
    }
   ],
   "source": [
    "result = await index_query(\"What is the main finding?\", \n",
    "                   output_writer=lmql.stream(variable=\"RESPONSE\"))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "# with documents\n",
    "# The main finding of the study is that the researchers used the dual-map analysis feature in CiteSpace 6.\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "# with semantic nodes\n",
    "# The main finding of the study is that the analysis primarily focused on the relationship between breast cancer and protein synthesis, including gene expression, translation, and apoptosis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... extracting info from patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Declarations under Rule 4.17: \n",
      "as to the identity of the inventor (Rule 4.17(i)) \n",
      "as to applicant's entitlement to apply for and be granted a \n",
      "patent (Rule 4.17(ii)) \n",
      "of inventorship (Rule 4.17(iv)) \n",
      "Published: \n",
      "with international search report (Art. 21(3)) \n",
      "before the expiration of the time limit for amending the \n",
      "claims and to be republished in the event of receipt of \n",
      "amendments (Rule 48.2(h)) \n",
      "(54) Title: DRILL AND TAP AND METHOD FOR PREOPERATIVE ASSESSMENT OF BONE QUALITY \n",
      "Fig. 1 \n",
      "42 \n",
      "(57) Abstract: A twist drill and bone tap each monitor torque while drilling or threading to assess jaw bone quality and a method for \n",
      "accessing bone quality prior to or while tapping into the bone during a dental implantation procedure. The twist drill for assessing \n",
      "bone quality includes a shank having a proximal section and a distal section. A mounting portion is formed in the proximal section \n",
      "and is adapted to connect with a torque monitoring device. A drill bit is connected to the distal section. The drill bit includes a cut - \n",
      "ting portion having at least one helical flute formed thereon. A drill point is located at an end of the cutting portion of the drill bit \n",
      "having a helix angle of between about 45 to about 55°, wherein a measurable torque is generated that can be assessed as a function \n",
      "of the quality of the bone material being drilled. A tap for assessing bone quality during tapping includes a shank having opposed \n",
      "ends. A mounting portion is formed in one end and is adapted to connect with a torque monitoring device. A cutting portion is dis - \n",
      "posed on the other end of the shank and has at least one helical thread formed thereon. A medial, non-cutting portion is disposed \n",
      "between the cutting portion and the mounting portion. The medial portion has a diameter less than a diameter of the cutting portion \n",
      "so as to minimize friction forces.\n",
      "----------------------------------------------------------\n",
      "WO 2014/076653 PCT/1B2013/060133 \n",
      "10/18 \n",
      "SHBES y \n",
      "x7 \n",
      "Ve \n",
      "New \n",
      "Qo \n",
      "wh & \n",
      "aS G \n",
      "~ “a \n",
      "N s nN g § \n",
      "nnn tntnnnnnnnnnnnnnnnnnannnnnnnnanananana a s = \n",
      "a3 3: : ; x \n",
      "SS 8 x \n",
      "tet hte tet \n",
      "sw SON \n",
      "SS NY RS = wy Ss \n",
      "S &S eo \n",
      "BSS se 8 \n",
      "— ‘ay \n",
      "Fig. 10A Fig. 108\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The invention described in the patent application is a twist drill and bone tap, along with a method for assessing bone quality during dental implantation procedures."
     ]
    }
   ],
   "source": [
    "result = await index_query(\"What is the invention?\", \n",
    "                   output_writer=lmql.stream(variable=\"RESPONSE\"))\n",
    "\n",
    "#------------------------------------------\n",
    "# with documents\n",
    "# The invention described in EP 2 671 601 A1 is an irrigation system suitable for rectal irrigation, which can be used for self-administration.\n",
    "# The invention described in the patent application is a twist drill and bone tap, along with a method for assessing bone quality during dental implantation procedures.\n",
    "\n",
    "#------------------------\n",
    "# with semantic nodes\n",
    "# The invention described in EP 2 671 601 A1 is not explicitly stated in the provided search report.\n",
    "# The invention described in the patent (Fig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "International application No. \n",
      "INTERNATIONAL SEARCH REPORT PCT/1B2013/060133 \n",
      "Box No. Il Observations where certain claims were found unsearchable (Continuation of item 2 of first sheet) \n",
      "This international search report has not been established in respect of certain claims under Article 17(2)(a) for the following reasons: \n",
      "1. Claims Nos.: - - \n",
      "because they relate to subject matter rot required to be searched by this Authority, namely: \n",
      "Rule 39.1(iv) PCT - Method for treatment of the human or animal body by \n",
      "surgery \n",
      "2. [ | Claims Nos.: \n",
      "because they relate to parts of the international application that do not comply with the prescribed requirements to such \n",
      "an extent that no meaningful international search can be carried out, specifically: \n",
      "3. [ | Claims Nos.: \n",
      "because they are dependent claims and are not drafted in accordance with the second and third sentences of Rule 6.4(a). \n",
      "Box No. Ill Observations where unity of invention is lacking (Continuation of item 3 of first sheet) \n",
      "This International Searching Authority found multiple inventions in this international application, as follows: \n",
      "1. As all required additional search fees were timely paid by the applicant, this international search report covers all searchable \n",
      "claims. \n",
      "2. [ | As all searchable claims could be searched without effort justifying an additional fees, this Authority did not invite payment of \n",
      "additional fees. \n",
      "3. As only some of the required additional search fees were timely paid by the applicant, this international search report covers \n",
      "only those claims for which fees were paid, specifically claims Nos. : \n",
      "4. [| No required additional search fees were timely paid by the applicant. Consequently, this international search report is \n",
      "restricted to the invention first mentioned in the claims; it is covered by claims Nos.: \n",
      "Remark on Protest The additional search fees were accompanied by the applicant's protest and, where applicable, the \n",
      "payment of a protest fee. \n",
      "The additional search fees were accompanied by the applicant's protest but the applicable protest \n",
      "fee was not paid within the time limit specified in the invitation. \n",
      "[ | No protest accompanied the payment of additional search fees. \n",
      "Form PCT/ISA/210 (continuation of first sheet (2)) (April 2005)\n",
      "----------------------------------------------------------\n",
      "Se Wag] \n",
      "— Ree \n",
      "WOT “did 997 “ald \n",
      "WwW BON \n",
      "oy \n",
      "SOKA SAN 4 \n",
      "NaN TAR ag, \n",
      "RS ees x \n",
      "ws ob een \n",
      "O9l “ald \n",
      "& 4 \n",
      "3 wating ¢ mM 4X % \n",
      "S sen \n",
      "MG \n",
      "wn geste Xs \n",
      "= \n",
      "we | eens \n",
      "ST/LT \n",
      "€£1090/€10THI/LOd €S99L0/FL0T OM\n",
      "----------------------------------------------------------\n",
      "Claim 1 could not be extracted directly from the provided patent document as it is not mentioned in the text."
     ]
    }
   ],
   "source": [
    "result = await index_query(\"Extract claim 1 from the patent?\", \n",
    "                   output_writer=lmql.stream(variable=\"RESPONSE\")\n",
    "                   )\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# with documents\n",
    "# To extract claim 1 from the patent, you can refer to the patent document itself.\n",
    "# Claim 1 could not be extracted directly from the provided patent document as it is not mentioned in the text.\n",
    "\n",
    "#-------------------------------------\n",
    "# with semantic nodes\n",
    "# To extract claim 1 from the patent, you would need to refer to the patent document itself as the relevant information provided does not contain the claim.\n",
    "# Claim 1 from the patent cannot be extracted directly from the provided information as the patent claims are not explicitly stated in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing node creation with different splitters provided by LlamaIndex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SentenceSplitter\n",
    "The SentenceSplitter attempts to split text in chunks while respecting the boundaries of sentences. \\\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can be defined globaly\n",
    "# Settings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "# # an be dafound per-index through transformations\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SentenceWindowNodeParser\n",
    "Splits all documents into individual sentences. The resulting nodes also contain the surrounding \"window\" of sentences around each node in the metadata.\\\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=2,  # how many sentences on either side to capture\n",
    "    window_metadata_key=\"window\", # the metadata key that holds the window of surrounding sentences\n",
    "    original_text_metadata_key=\"original_sentence\", # the metadata key that holds the original sentence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SemanticSplitterNodeParser\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. HierarchicalNodeParser\n",
    "Input is chunked into several hierarchies of chunk sizes, with each node containing a reference to it's parent node. https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/ \\\n",
    "When combined with the AutoMergingRetriever, this enables us to automatically replace retrieved nodes with their parents when a majority of children are retrieved. https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever/ (conclusion in tutorial that output quality similar to non hierarchical approach...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk into parent, child, grandchild (leaf) nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "\n",
    "splitter = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128] # chunk size parent, child, grandchild\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate grandchild nodes from root nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes\n",
    "\n",
    "base_nodes = get_leaf_nodes(nodes)\n",
    "root_nodes = get_root_nodes(nodes)\n",
    "\n",
    "len(base_nodes), len(root_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all nodes into SimpleDocumentStore and only leaf nodes into VectoreStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore) # define storage context (will include vector store by default too)\n",
    "\n",
    "## Load index into vector index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "base_index = VectorStoreIndex(\n",
    "    base_nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=3)\n",
    "retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n",
    "\n",
    "# query_str = (\"What is the title of the article?\")\n",
    "query_str = (\"What is the main topic of the article?\")\n",
    "\n",
    "nodes = retriever.retrieve(query_str)\n",
    "base_nodes = base_retriever.retrieve(query_str)\n",
    "\n",
    "len(nodes), len(base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "import matplotlib\n",
    "\n",
    "for node in base_nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TokenTextSplitter https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: seem to be the same output: nodes.get_content(), nodes.text, nodes.get_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
