{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMQI with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTENT:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "CONCLUSIONS:\n",
    "* LMQL does not seem to support Pydantic, instead @dataclasse is used\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStoreIndex with documents vs nodes tested on scientific article and patent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lmql.ai/docs/latest/lib/integrations/llama_index.html - outdated example with regard to query/query_engine \\\n",
    "https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: to runt the notebook\n",
    "1. Remove 'local:' from llm = lmql.model(\"local:llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\"), \n",
    "1. start a service in terminal with: lmql serve-model llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf --verbose True --n_gpu_layers 20 --n_ctx 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dorota/LLM-diploma-project/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lmql\n",
    "from llama_index.core import GPTVectorStoreIndex, VectorStoreIndex, SimpleDirectoryReader, ServiceContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama.cpp endpoint: https://lmql.ai/docs/models/llama.cpp.html#running-without-a-model-server\n",
    "# tokenizer.model from https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main\n",
    "\n",
    "llm = lmql.model(\"llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\", n_gpu_layers=10, n_ctx=0, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all documents from assigned folder\n",
    "documents = SimpleDirectoryReader(input_files=[\"/home/dorota/LLM-diploma-project/00_concept_tests/data/40001_2023_Article_1364.pdf\"]).load_data() # -> list of Document objects with 1 doc/page in article with metadata and tags (documents[0].text)\n",
    "# documents = SimpleDirectoryReader(input_files=[\"/home/dorota/LLM-diploma-project/00_concept_tests/data/patents/EP2671601A1.pdf\"]).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables to create vector embeddings for text nodes\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2').encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 17/17 [00:00<00:00, 923.90it/s]\n",
      "Generating embeddings:   0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 24/24 [00:00<00:00, 183.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True) #[0:1] # index = VectorStoreIndex(nodes)\n",
    "\n",
    "Settings.llm = None # =None to enable correct setting in query_engine\n",
    "query_engine = index.as_query_engine(streaming=True, llm=None) # llm=None sets llm to Settings.llm thus defined as None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is the main topic of the article?\"\n",
    "# response = query_engine.query(question)\n",
    "# response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.source_nodes[0].node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with Semantic nodes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=99, embed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "index= VectorStoreIndex(nodes)\n",
    "\n",
    "Settings.llm = None\n",
    "query_engine = index.as_query_engine(streaming=True, llm=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_top_k = 4\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "async def index_query(question: str):\n",
    "    '''lmql\n",
    "    \"You are a QA bot that helps users answer questions.\\n\"\n",
    "    \n",
    "    # ask the question\n",
    "    \"Question: {question}\\n\"\n",
    "\n",
    "    # look up and insert relevant information into the context\n",
    "    response = query_engine.query(question)\n",
    "    for s in response.source_nodes:\n",
    "        print(s.node.get_text())\n",
    "        print('----------------------------------------------------------')\n",
    "    information = \"\\n\\n\".join([s.node.get_text() for s in response.source_nodes])\n",
    "    \"\\nRelevant Information: {information}\\n\"\n",
    "    \n",
    "    # generate a response\n",
    "    \"Your response based on relevant information:[RESPONSE]\" where STOPS_AT(RESPONSE, \".\")\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... extracting info from scientific article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 14 of 21 Xu et al. European Journal of Medical Research          (2023) 28:461 \n",
      "and new knowledge that emerged. \n",
      "----------------------------------------------------------\n",
      "Employing a segmentation process, topics exhibit -\n",
      "ing akin clusters were deftly allocated to cohesive areas, \n",
      "thereby engendering a heightened sense of organization \n",
      "and a more comprehensive grasp of the underlying data \n",
      "(Fig.  8a). In this analysis, a keyword co-occurrence analy -\n",
      "sis was conducted to identify the most frequently appear -\n",
      "ing terms. The analysis included five keywords: “breast \n",
      "cancer” with 1339 occurrences, “expression” with 831 \n",
      "occurrences, “cancer” with 407 occurrences, “protein” \n",
      "with 358 occurrences, and “translation” with 350 occur -\n",
      "rences. These results suggest that the analysis primarily \n",
      "focused on the relationship between breast cancer and \n",
      "protein synthesis, including gene expression, translation, \n",
      "and apoptosis. The aim of this analysis was to identify the \n",
      "most frequent keywords related to breast cancer-related \n",
      "Fig. 7 Co-citation network map of authors of breast cancer-related protein synthesis for the period 2003 to 2022\n",
      "----------------------------------------------------------\n",
      "The main finding of the study is that the analysis primarily focused on the relationship between breast cancer and protein synthesis, including gene expression, translation, and apoptosis."
     ]
    }
   ],
   "source": [
    "result = await index_query(\"What is the main finding?\", \n",
    "                   output_writer=lmql.stream(variable=\"RESPONSE\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... extracting info from patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP 2 671 601 A1 \n",
      "des brevets MN EPO FORM 1503 03.82 (P04C01) Europaisches \n",
      "Patentamt \n",
      "European \n",
      "9) Pre te EUROPEAN SEARCH REPORT \n",
      "DOCUMENTS CONSIDERED TO BE RELEVANT Application Number \n",
      "EP 12 17 1154 \n",
      "Category Citation of document with indication, where appropriate, \n",
      "of relevant passages Relevant \n",
      "to claim CLASSIFICATION OF THE \n",
      "APPLICATION (IPC) \n",
      "X GB 238 038 A (ISIDOR JULIUS FRANKENSTEIN) \n",
      "13 August 1925 (1925-08-13) \n",
      "* the whole document * \n",
      "US 5 498 237 A (KELLER RICHARD D [US]) \n",
      "12 March 1996 (1996-03-12) \n",
      "* column 4, line 18 - line 59; figures 4-7 * \n",
      "US 1 473 979 A (LEO SIMMONS) \n",
      "13 November 1923 (1923-11-13) \n",
      "* the whole document * \n",
      "US 2003/073963 Al (FALCONER MALCOLM IAN \n",
      "[GB]) 17 April 2003 (2003-04-17) \n",
      "* paragraph [0009] * \n",
      "US 7 150 739 B2 (O'NEIL ALEXANDER GEORGE \n",
      "BRIAN [AU]) 19 December 2006 (2006-12-19) \n",
      "* paragraph [0026]; figure 1 * \n",
      "WO 98/23312 Al (COLOPLAST AS [DK]; \n",
      "ALEXANDERSEN MORTEN BAY [DK]) \n",
      "4 June 1998 (1998-06-04) \n",
      "* 14 measuring means; \n",
      "page 8, line 15 - line 19; figure 1 * \n",
      "The present search report has been drawn up for all claims 1-3,7-15 \n",
      "1-6 \n",
      "1-4 \n",
      "1-4 \n",
      "1-3,5,6 INV. \n",
      "A61M3/02 \n",
      "12,13 TECHNICAL FIELDS \n",
      "SEARCHED (IPC) \n",
      "A61M \n",
      "Place of search Date of completion of the search \n",
      "The Hague 19 November 2012 Van Examiner \n",
      "Veen, Jennifer \n",
      "CATEGORY OF CITED DOCUMENTS T : theory or principle underlying the invention \n",
      "E: earlier patent document, but published on, or \n",
      "X : particularly relevant if taken alone after the filing date \n",
      "D: document cited in the application \n",
      "L: document cited for other reasons \n",
      "A:technologicalbackground i ceteterenentetavaceteeeteeee Y : particularly relevant if combined with another \n",
      "document of the same category \n",
      "©: non-written disclosure &: member of the same patent family, corresponding \n",
      "P : intermediate document document \n",
      "15\n",
      "----------------------------------------------------------\n",
      "EP 2 671 601 A1 \n",
      "11\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP 2 671 601 A1 is a European patent document with the title not provided in the given information."
     ]
    }
   ],
   "source": [
    "result = await index_query(\"What is the invention?\", \n",
    "                   output_writer=lmql.stream(variable=\"RESPONSE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP 2 671 601 A1 \n",
      "REFERENCES CITED IN THE DESCRIPTION \n",
      "This list of references cited by the applicant is for the reader’s convenience only. It does not form part of the European \n",
      "patent document. Even though great care has been taken in compiling the references, errors or omissions cannot be \n",
      "excluded and the EPO disclaims all liability in this regard. \n",
      "Patent documents cited in the description \n",
      "« WO 2008087220 A [0005] ¢ WO 2011023196 A [0005] \n",
      "* WO 2009092380 A [0005] « WO 03030968 A [0005] \n",
      "¢« WO 03030969 A [0005] \n",
      "17\n",
      "----------------------------------------------------------\n",
      "EP 2 671 601 A1 \n",
      "11\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to extract claim 1 directly from the text you've provided."
     ]
    }
   ],
   "source": [
    "result = await index_query(\"Extract claim 1 from the patent?\", \n",
    "                   output_writer=lmql.stream(variable=\"RESPONSE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMQL output to a  @dataclass for strucutred output\n",
    "https://lmql.ai/blog/ \\\n",
    "https://www.timlrx.com/blog/generating-structured-output-from-llms#lmql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial example without provided context for LLM generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Ingredient:\n",
    "    name: str\n",
    "    weight_in_grams: int\n",
    "\n",
    "@dataclass\n",
    "class Recipe:\n",
    "    recipe_name: str\n",
    "    servings: int\n",
    "    ingredient1: Ingredient\n",
    "    ingredient2: Ingredient\n",
    "    ingredient3: Ingredient\n",
    "    ingredient4: Ingredient\n",
    "    ingredient5: Ingredient\n",
    "    ingredient6: Ingredient\n",
    "    ingredient7: Ingredient\n",
    "    ingredient8: Ingredient\n",
    "    # list not supported...\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "async def spaghetti():\n",
    "    '''lmql\n",
    "    \"Spaghetti bolognese recipe for a family of 4.\"\n",
    "    \"[RECIPE_DATA]\\\\n\" where type(RECIPE_DATA) is Recipe\n",
    "    return RECIPE_DATA\n",
    "    '''\n",
    "\n",
    "result = await spaghetti()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recipe(recipe_name='Spaghetti Bolognese', servings=4, ingredient1=Ingredient(name='Spaghetti', weight_in_grams=454), ingredient2=Ingredient(name='Ground Beef', weight_in_grams=454), ingredient3=Ingredient(name='Olive Oil', weight_in_grams=2), ingredient4=Ingredient(name='Onion', weight_in_grams=150), ingredient5=Ingredient(name='Garlic', weight_in_grams=3), ingredient6=Ingredient(name='Tomato Sauce', weight_in_grams=845), ingredient7=Ingredient(name='Tomato Paste', weight_in_grams=113), ingredient8=Ingredient(name='Salt', weight_in_grams=2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_name': 'Spaghetti Bolognese',\n",
       " 'servings': 4,\n",
       " 'ingredient1': Ingredient(name='Spaghetti', weight_in_grams=450),\n",
       " 'ingredient2': Ingredient(name='Olive Oil', weight_in_grams=2),\n",
       " 'ingredient3': Ingredient(name='Onion', weight_in_grams=150),\n",
       " 'ingredient4': Ingredient(name='Garlic', weight_in_grams=2),\n",
       " 'ingredient5': Ingredient(name='Carrots', weight_in_grams=100),\n",
       " 'ingredient6': Ingredient(name='Celery', weight_in_grams=50),\n",
       " 'ingredient7': Ingredient(name='Ground Beef', weight_in_grams=400),\n",
       " 'ingredient8': Ingredient(name='Tomato Sauce', weight_in_grams=800)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ingredient(name='Spaghetti', weight_in_grams=450)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.ingredient1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingredient1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Recipe.__dataclass_fields__['ingredient1'].name #Recipe.__dataclass_fields__ is a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_name': str,\n",
       " 'servings': int,\n",
       " 'ingredient1': str,\n",
       " 'ingredient2': str,\n",
       " 'ingredient3': str}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Recipe.__annotations__ # dict with name:type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own example with recipe with provided context for LLM generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "recipe = \"\"\"Ingredients for 4 servings:\n",
    "• 1 cup all-purpose flour\n",
    "• 3 tablespoons granulated sugar\n",
    "• 1 teaspoon baking powder\n",
    "• 1/2 teaspoon baking soda\n",
    "• 1/2 teaspoon salt\n",
    "• 1 cup milk\n",
    "• 1 egg\n",
    "• 3 tablespoons unsalted butter, melted\n",
    "• Toppings of your choice (fresh fruit, whipped cream, syrup, etc.)\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Recipe:\n",
    "    recipe_name: str\n",
    "    servings: int\n",
    "    ingredient1: str\n",
    "    ingredient2: str\n",
    "    ingredient3: str\n",
    "\n",
    "@lmql.query(model=llm, verbose=False)\n",
    "async def get_recipe(recipe):\n",
    "    '''lmql\n",
    "    \"{recipe}\"\n",
    "    \"[RECIPE_DATA]\\\\n\" where type(RECIPE_DATA) is Recipe\n",
    "    return RECIPE_DATA\n",
    "    '''\n",
    "\n",
    "result = await get_recipe(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_name': 'Buttermilk Pancakes',\n",
       " 'servings': 4,\n",
       " 'ingredient1': '1 cup all-purpose flour',\n",
       " 'ingredient2': '3 tablespoons granulated sugar',\n",
       " 'ingredient3': '1 teaspoon baking powder'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "recipe = \"\"\"Ingredients:\n",
    "• 1 cup all-purpose flour\n",
    "• 3 tablespoons granulated sugar\n",
    "• 1 teaspoon baking powder\n",
    "• 1/2 teaspoon baking soda\n",
    "• 1/2 teaspoon salt\n",
    "• 1 cup milk\n",
    "• 1 egg\n",
    "• 3 tablespoons unsalted butter, melted\n",
    "• Toppings of your choice (fresh fruit, whipped cream, syrup, etc.)\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Recipe:\n",
    "    recipe_name: str\n",
    "    servings: int\n",
    "    ingredient1: str\n",
    "    ingredient2: str\n",
    "    ingredient3: str\n",
    "\n",
    "field_descriptions = [\n",
    "    \"Generate a short recipe name\",\n",
    "    \"Generate number of servings based on ingredient amounts\",\n",
    "    \"Extract ingredient name only\",\n",
    "    \"Extract ingredient amount only\",\n",
    "    \"Extract ingredient name and ingredient amount\",\n",
    "]\n",
    "\n",
    "field_prompting = \"\"\n",
    "for field_name, field_descr in zip(Recipe.__annotations__.keys(), field_descriptions):\n",
    "    field_prompting += \"For field \" + \"'\" + field_name + \"'\" + \" \" + \"follow these instructions: \" + field_descr + \"\\n \"\n",
    "\n",
    "@lmql.query(model=llm, verbose=False)\n",
    "async def get_recipe(recipe, field_prompting):\n",
    "    '''lmql\n",
    "    \"{recipe}\"\n",
    "    \"{field_prompting}\"\n",
    "    \"[RECIPE_DATA]\\\\n\" where type(RECIPE_DATA) is Recipe\n",
    "    return RECIPE_DATA\n",
    "    '''\n",
    "\n",
    "result = await get_recipe(recipe, field_prompting)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# output result.__dict__:\n",
    "\n",
    "# {'recipe_name': 'Buttermilk Pancakes',\n",
    "# 'servings': 4,\n",
    "# 'ingredient1': 'all-purpose flour',\n",
    "# 'ingredient2': '3 tbsp',\n",
    "# 'ingredient3': '1 cup'}\n",
    "\n",
    "# -> ingredient3 should be name and amount\n",
    "# NOTE: bug with infinite List generation thus can not use ingredients: List(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_name': 'Buttermilk Pancakes',\n",
       " 'servings': 4,\n",
       " 'ingredient1': 'all-purpose flour',\n",
       " 'ingredient2': '3 tbsp',\n",
       " 'ingredient3': '1 cup'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own example with article text as context for LLM..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader \n",
    "  \n",
    "reader = PdfReader('/home/dorota/LLM-diploma-project/00_concept_tests/data/40001_2023_Article_1364.pdf') \n",
    "num_pages = len(reader.pages)\n",
    "TEXT = \"\"\n",
    "for page_num in range(1): #change to range(num_pages) for whole document\n",
    "    page = reader.pages[page_num]  \n",
    "    TEXT += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??????????? hur utöka conext fönstret så kan ta med hela artikelln och inte bara 1a sidan???????????????????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... with @dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Visualization of breast cancer -related protein synthesis from the perspective of bibliometric analysis',\n",
       " 'authors': 'Jiawei Xu, Chengdong Yu, Xiaoqiang Zeng, Weifeng Tang, Siyi Xu, Lei Tang, Yanxiao Huang, Zhengkui Sun, Tenghua Yu',\n",
       " 'pub_year': 202,\n",
       " 'key_words': 'Breast cancer, Bibliometric analysis, Protein synthesis, Expression, Cancer, Protein, Translation',\n",
       " 'summary': \"This article provides insights into the research on breast cancer and protein synthesis through bibliometric analysis. The analysis reveals a steady increase in publications, with most articles published in oncology or biology-related journals. Keyword analysis shows that 'breast cancer,' 'expression,' 'cancer,' 'protein,' and 'translation' are the most commonly researched topics. The research focuses on the relationship between protein expression in breast cancer and tumor development and treatment.\",\n",
       " 'research_area': 'Breast cancer research, Protein synthesis research',\n",
       " 'quality': 'GOOD',\n",
       " 'quality_reason': 'The article provides a comprehensive analysis of the literature on breast cancer and protein synthesis, using reliable data sources and valid analytical methods.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Metadata:\n",
    "    title: str\n",
    "    authors: str\n",
    "    pub_year: int\n",
    "    key_words: str\n",
    "    summary: str\n",
    "    research_area: str\n",
    "    quality: str\n",
    "    quality_reason: str\n",
    "\n",
    "field_descriptions = [\n",
    "    \"extract title from article\",\n",
    "    \"extract authors from article\",\n",
    "    \"extract publication year\",\n",
    "    \"generate 5 new key words based on content in Abstract\",\n",
    "    \"generate summary in 3 sentences\",\n",
    "    \"generate 1 main research area described in article\",\n",
    "    \"select 1 value from ['GOOD', 'BAD', 'EXCELLENT', 'CAN NOT SET QUALITY'] to define quality of article\",\n",
    "    \"describe reason for chosen quality_score in 1 sentece\",\n",
    "]\n",
    "\n",
    "field_prompting = \"\"\n",
    "for field_name, field_descr in zip(Metadata.__annotations__.keys(), field_descriptions):\n",
    "    field_prompting += \"For field \" + \"'\" + field_name + \"'\" + \" \" + \"follow these instructions: \" + field_descr + \"\\n \"\n",
    "\n",
    "@lmql.query(model=llm, verbose=False)\n",
    "async def get_metadata(TEXT, field_prompting):\n",
    "    '''lmql\n",
    "    \"{TEXT}\"\n",
    "    \"{field_prompting}\"\n",
    "    \"[METADATA]\\\\n\" where type(METADATA) is Metadata\n",
    "    return METADATA\n",
    "    '''\n",
    "\n",
    "result = await get_metadata(TEXT, field_prompting)\n",
    "result.__dict__\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# output result.__dict__:\n",
    "#{'title': 'Visualization of breast cancer -related protein synthesis from the perspective of bibliometric analysis',\n",
    "# 'authors': 'Jiawei Xu, Chengdong Yu, Xiaoqiang Zeng, Weifeng Tang, Siyi Xu, Lei Tang, Yanxiao Huang, Zhengkui Sun, Tenghua Yu',\n",
    "# 'pub_year': 202,\n",
    "# 'key_words': 'Breast cancer, Bibliometric analysis, Protein synthesis, Expression, Cancer, Protein, Translation',\n",
    "# 'summary': \"This article provides insights into the research on breast cancer and protein synthesis through bibliometric analysis. The analysis reveals a steady increase in publications, with most articles published in oncology or biology-related journals. Keyword analysis shows that 'breast cancer,' 'expression,' 'cancer,' 'protein,' and 'translation' are the most commonly researched topics. The research focuses on the relationship between protein expression in breast cancer and tumor development and treatment.\",\n",
    "# 'research_area': 'Breast cancer research, Protein synthesis research',\n",
    "# 'quality': 'GOOD',\n",
    "# 'quality_reason': 'The article provides a comprehensive analysis of the literature on breast cancer and protein synthesis, using reliable data sources and valid analytical methods.'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For field 'title' follow these instructions: extract title from article\\n For field 'authors' follow these instructions: extract authors from article\\n For field 'pub_year' follow these instructions: extract publication year\\n For field 'key_words' follow these instructions: generate 5 new key words based on content in Abstract\\n For field 'summary' follow these instructions: generate summary in 3 sentences\\n For field 'research_area' follow these instructions: generate 1 main research area described in article\\n For field 'quality' follow these instructions: select 1 value from ['GOOD', 'BAD', 'EXCELLENT', 'CAN NOT SET QUALITY'] to define quality of article\\n For field 'quality_reason' follow these instructions: describe reason for chosen quality_score in 1 sentece\\n \""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... with @dataclass and constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For field 'pub_year' follow these instructions: extract publication year - [PUB_YEAR]\n",
      "\n",
      "['where len(PUB_YEAR) == 4']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pub_year': 202}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Metadata:\n",
    "\n",
    "    pub_year: int\n",
    "\n",
    "\n",
    "field_descriptions = [\n",
    "\n",
    "    \"extract publication year\",\n",
    "\n",
    "]\n",
    "\n",
    "for field_name, field_descr in zip(Metadata.__annotations__.keys(), field_descriptions):\n",
    "    field_prompting = f\"For field '{field_name}' follow these instructions: {field_descr} - [{field_name.upper()}]\\n\"\n",
    "    print(field_prompting)\n",
    "\n",
    "@lmql.query(model=llm, verbose=False)\n",
    "async def get_metadata(TEXT, field_prompting):\n",
    "    '''lmql\n",
    "    \"{TEXT}\"\n",
    "    constraints = [\"where len(PUB_YEAR) == 4\"]\n",
    "    print(constraints)\n",
    "    \"{field_prompting}\" + constraints[0]\n",
    "    \"[METADATA]\\\\n\" where type(METADATA) is Metadata\n",
    "    return METADATA\n",
    "    '''\n",
    "\n",
    "result = await get_metadata(TEXT, field_prompting)\n",
    "result.__dict__\n",
    "\n",
    "# hur lägga på instruktioner för varje fält i prompten så att de inte bara är del av strängen?????????????\n",
    "# REGEX(PUB_YEAR, r\"[0-9]{4}\")\n",
    "# utöka contextfönstret så hela artikeln kommer med?????????????????????????????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... with Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    title: str = Field(..., description=\"extract title from article\")\n",
    "    authors: str = Field(..., description=\"extract authors from article\")\n",
    "    pub_year: int = Field(..., description=\"extract publication year\")\n",
    "    key_words: str = Field(..., description=\"generate 5 new key words based on content in Abstract\")\n",
    "    summary: str = Field(..., description=\"generate summary in 3 sentences\")\n",
    "    research_area: str = Field(..., description=\"generate 1 main research area described in article\")\n",
    "    quality: str = Field(..., description=\"select one value from provided examples to define quality of article\", examples=['GOOD', 'BAD', 'EXCELLENT', 'CAN NOT SET QUALITY'])\n",
    "    quality_reason: str = Field(..., description=\"describe reason for chosen quality_score in 1 sentece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': FieldInfo(annotation=str, required=True, description='extract title from article'),\n",
       " 'authors': FieldInfo(annotation=str, required=True, description='extract authors from article'),\n",
       " 'pub_year': FieldInfo(annotation=int, required=True, description='extract publication year'),\n",
       " 'key_words': FieldInfo(annotation=str, required=True, description='generate 5 new key words based on content in Abstract'),\n",
       " 'summary': FieldInfo(annotation=str, required=True, description='generate summary in 3 sentences'),\n",
       " 'research_area': FieldInfo(annotation=str, required=True, description='generate 1 main research area described in article'),\n",
       " 'quality': FieldInfo(annotation=str, required=True, description='select one value from provided examples to define quality of article', examples=['GOOD', 'BAD', 'EXCELLENT', 'CAN NOT SET QUALITY']),\n",
       " 'quality_reason': FieldInfo(annotation=str, required=True, description='describe reason for chosen quality_score in 1 sentece')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metadata.model_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    title: str = Field(..., description=\"extract title from article\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('title', FieldInfo(annotation=str, required=True, description='extract title from article'))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Metadata.model_fields.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "extract title from article\n",
      "For field 'title' follow these instructions: extract title from article\n",
      " \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Failed to resolve variable 'JSON' in LMQL query.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 17\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;129m@lmql\u001b[39m\u001b[38;5;241m.\u001b[39mquery(model\u001b[38;5;241m=\u001b[39mllm, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_metadata\u001b[39m(TEXT, field_prompting):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''lmql\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"{TEXT}\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    \"{field_prompting}\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"[METADATA]\\\\n\" where type(METADATA) is JSON\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    return METADATA\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_metadata(TEXT, field_prompting)\n\u001b[1;32m     18\u001b[0m result\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n",
      "File \u001b[0;32m~/LLM-diploma-project/venv/lib/python3.10/site-packages/lmql/runtime/lmql_runtime.py:211\u001b[0m, in \u001b[0;36mLMQLQueryFunction.__acall__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__acall__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlmql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mruntime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterpreter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptInterpreter\n\u001b[0;32m--> 211\u001b[0m     query_kwargs, runtime_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     forced_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mor\u001b[39;00m runtime_args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    214\u001b[0m     interpreter \u001b[38;5;241m=\u001b[39m PromptInterpreter(force_model\u001b[38;5;241m=\u001b[39mforced_model, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m~/LLM-diploma-project/venv/lib/python3.10/site-packages/lmql/runtime/lmql_runtime.py:196\u001b[0m, in \u001b[0;36mLMQLQueryFunction.make_kwargs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# disable this check for now, as dynamic variable resolution cannot always be checked at compile time (e.g. import * from module)\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_to_resolve) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to resolve variable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m failed_to_resolve[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in LMQL query.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_to_resolve) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to resolve variables in LMQL query: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(failed_to_resolve)))\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to resolve variable 'JSON' in LMQL query."
     ]
    }
   ],
   "source": [
    "\n",
    "for key, value in Metadata.model_fields.items():\n",
    "    print(key)\n",
    "    print(value.description)\n",
    "    field_prompting = \"For field \" + \"'\" + field_name + \"'\" + \" \" + \"follow these instructions: \" + value.description + \"\\n \"\n",
    "\n",
    "print(field_prompting)\n",
    "\n",
    "@lmql.query(model=llm, verbose=False)\n",
    "async def get_metadata(TEXT, field_prompting):\n",
    "    '''lmql\n",
    "    \"{TEXT}\"\n",
    "    \"{field_prompting}\"\n",
    "    \"[METADATA]\\\\n\" where type(METADATA) is Metadata\n",
    "    return METADATA\n",
    "    '''\n",
    "\n",
    "result = await get_metadata(TEXT, field_prompting)\n",
    "result.__dict__\n",
    "\n",
    "# ??????????? can not output according to Pydantic class Metadata. have to use dataclass as input instead of pydantic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: generate field_prompting from Pydantic dictionary and tranform reulting result.__dict__ to Pydantic\n",
    "#TODO: create a loop to include SQL restrictions\n",
    "#TODO: kör med olika noder via VectorStore Semantic uppdelning på noder\n",
    "#TODO: kan inte köra hela artikeln även om n_ctx=0 så testa att utöka max_content (eller vad den nu klagade på) eller kör med olika noder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Chain of thought example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The day one week ago was the 5th of June.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "def chain_of_thought(question):\n",
    "    '''lmql\n",
    "    # Q&A prompt template\n",
    "    \"Q: {question}\\n\"\n",
    "    \"A: Let's think step by step.\\n\"\n",
    "    \"[REASONING]\"\n",
    "    \"Thus, the answer is:[ANSWER].\" where STOPS_AT(ANSWER, \".\")\n",
    "\n",
    "    # return just the ANSWER to the caller\n",
    "    return ANSWER.strip()\n",
    "    '''\n",
    "\n",
    "result = chain_of_thought('Today is the 12th of June, what day was it 1 week ago?')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data with SimpleDirectoryReader\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/\n",
    "\n",
    "more readers availble at https://llamahub.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You can specify a function that will read each file and extract metadata that gets attached to the resulting Document\n",
    "# def get_meta(file_path):\n",
    "#     return {\"foo\": \"bar\", \"file_path\": file_path}\n",
    "\n",
    "\n",
    "# SimpleDirectoryReader(input_files=[\"/home/dorota/LLM-diploma-project/00_concept_tests/data/40001_2023_Article_1364.pdf\"], file_metadata=get_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # additional possibilities with SimpleDirectoryReader\n",
    "# documents = SimpleDirectoryReader(input_dir=\"/home/dorota/LLM-diploma-project/00_concept_tests/data\", recursive=True).load_data(num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create nodes with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SentenceSplitter\n",
    "The SentenceSplitter attempts to split text in chunks while respecting the boundaries of sentences. \\\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can be defined globaly\n",
    "# Settings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "# # an be dafound per-index through transformations\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SentenceWindowNodeParser\n",
    "Splits all documents into individual sentences. The resulting nodes also contain the surrounding \"window\" of sentences around each node in the metadata.\\\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=2,  # how many sentences on either side to capture\n",
    "    window_metadata_key=\"window\", # the metadata key that holds the window of surrounding sentences\n",
    "    original_text_metadata_key=\"original_sentence\", # the metadata key that holds the original sentence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SemanticSplitterNodeParser\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. HierarchicalNodeParser\n",
    "Input is chunked into several hierarchies of chunk sizes, with each node containing a reference to it's parent node. https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/ \\\n",
    "When combined with the AutoMergingRetriever, this enables us to automatically replace retrieved nodes with their parents when a majority of children are retrieved. https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever/ (conclusion in tutorial that output quality similar to non hierarchical approach...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk into parent, child, grandchild (leaf) nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "\n",
    "splitter = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128] # chunk size parent, child, grandchild\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate grandchild nodes from root nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes\n",
    "\n",
    "base_nodes = get_leaf_nodes(nodes)\n",
    "root_nodes = get_root_nodes(nodes)\n",
    "\n",
    "len(base_nodes), len(root_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all nodes into SimpleDocumentStore and only leaf nodes into VectoreStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore) # define storage context (will include vector store by default too)\n",
    "\n",
    "## Load index into vector index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "base_index = VectorStoreIndex(\n",
    "    base_nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=3)\n",
    "retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n",
    "\n",
    "# query_str = (\"What is the title of the article?\")\n",
    "query_str = (\"What is the main topic of the article?\")\n",
    "\n",
    "nodes = retriever.retrieve(query_str)\n",
    "base_nodes = base_retriever.retrieve(query_str)\n",
    "\n",
    "len(nodes), len(base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "import matplotlib\n",
    "\n",
    "for node in base_nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TokenTextSplitter https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: seem to be the same output: nodes.get_content(), nodes.text, nodes.get_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
