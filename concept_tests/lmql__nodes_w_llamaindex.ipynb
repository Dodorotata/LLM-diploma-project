{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMQI with LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONTENT:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "CONCLUSIONS:\n",
    "* LMQL does not seem to support Pydantic, instead @dataclasse is used\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorStoreIndex with documents vs nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lmql.ai/docs/latest/lib/integrations/llama_index.html - outdated example with regard to query/query_engine \\\n",
    "https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: to runt the notebook\n",
    "1. Remove 'local:' from llm = lmql.model(\"local:llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\"), \n",
    "1. start a service in terminal with: lmql serve-model llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf --verbose True --n_gpu_layers 20 --n_ctx 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "from llama_index.core import GPTVectorStoreIndex, VectorStoreIndex, SimpleDirectoryReader, ServiceContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama.cpp endpoint: https://lmql.ai/docs/models/llama.cpp.html#running-without-a-model-server\n",
    "# tokenizer.model from https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/tree/main\n",
    "\n",
    "llm = lmql.model(\"llama.cpp:/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", tokenizer=\"mistralai/Mistral-7B-Instruct-v0.2\", n_gpu_layers=10, n_ctx=0, verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all documents from assigned folder\n",
    "documents = SimpleDirectoryReader(input_files=[\"/home/dorota/LLM-diploma-project/00_concept_tests/data/40001_2023_Article_1364.pdf\"]).load_data() # -> list of Document objects with 1 doc/page in article with metadata and tags (documents[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global variables to create vector embeddings for text nodes\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2').encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 21/21 [00:00<00:00, 251.36it/s]\n",
      "Generating embeddings: 100%|██████████| 47/47 [00:00<00:00, 191.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress=True) #[0:1] # index = VectorStoreIndex(nodes)\n",
    "\n",
    "Settings.llm = None # =None to enable correct setting in query_engine\n",
    "query_engine = index.as_query_engine(streaming=True, llm=None) # llm=None sets llm to Settings.llm thus defined as None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"What is the main topic of the article?\"\n",
    "# response = query_engine.query(question)\n",
    "# response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.source_nodes[0].node.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with Semantic nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
    ")\n",
    "\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "index= VectorStoreIndex(nodes)\n",
    "\n",
    "Settings.llm = None\n",
    "query_engine = index.as_query_engine(streaming=True, llm=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_top_k = 2\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "async def index_query(question: str):\n",
    "    '''lmql\n",
    "    \"You are a QA bot that helps users answer questions.\\n\"\n",
    "    \n",
    "    # ask the question\n",
    "    \"Question: {question}\\n\"\n",
    "\n",
    "    # look up and insert relevant information into the context\n",
    "    response = query_engine.query(question)\n",
    "    for s in response.source_nodes:\n",
    "        print(s.node.get_text())\n",
    "        print('----------------------------------------------------------')\n",
    "    information = \"\\n\\n\".join([s.node.get_text() for s in response.source_nodes])\n",
    "    \"\\nRelevant Information: {information}\\n\"\n",
    "    \n",
    "    # generate a response\n",
    "    \"Your response based on relevant information:[RESPONSE]\" where len(RESPONSE) < 200 and STOPS_AT(RESPONSE, \".\")\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 14 of 21 Xu et al. European Journal of Medical Research          (2023) 28:461 \n",
      "and new knowledge that emerged. \n",
      "----------------------------------------------------------\n",
      "Employing a segmentation process, topics exhibit -\n",
      "ing akin clusters were deftly allocated to cohesive areas, \n",
      "thereby engendering a heightened sense of organization \n",
      "and a more comprehensive grasp of the underlying data \n",
      "(Fig.  8a). In this analysis, a keyword co-occurrence analy -\n",
      "sis was conducted to identify the most frequently appear -\n",
      "ing terms. The analysis included five keywords: “breast \n",
      "cancer” with 1339 occurrences, “expression” with 831 \n",
      "occurrences, “cancer” with 407 occurrences, “protein” \n",
      "with 358 occurrences, and “translation” with 350 occur -\n",
      "rences. These results suggest that the analysis primarily \n",
      "focused on the relationship between breast cancer and \n",
      "protein synthesis, including gene expression, translation, \n",
      "and apoptosis. The aim of this analysis was to identify the \n",
      "most frequent keywords related to breast cancer-related \n",
      "Fig. 7 Co-citation network map of authors of breast cancer-related protein synthesis for the period 2003 to 2022\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main finding of the study is that the analysis primarily focused on the relationship between breast cancer and protein synthesis, including gene expression, translation, and apoptosis."
     ]
    }
   ],
   "source": [
    "result = await index_query(\"What is the main finding?\", \n",
    "                   output_writer=lmql.stream(variable=\"RESPONSE\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMQL output to a  @dataclass for strucutred output\n",
    "https://lmql.ai/blog/ \\\n",
    "https://www.timlrx.com/blog/generating-structured-output-from-llms#lmql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Ingredient:\n",
    "    name: str\n",
    "    weight_in_grams: int\n",
    "\n",
    "@dataclass\n",
    "class Recipe:\n",
    "    recipe_name: str\n",
    "    servings: int\n",
    "    ingredient1: Ingredient\n",
    "    ingredient2: Ingredient\n",
    "    ingredient3: Ingredient\n",
    "    ingredient4: Ingredient\n",
    "    ingredient5: Ingredient\n",
    "    ingredient6: Ingredient\n",
    "    ingredient7: Ingredient\n",
    "    ingredient8: Ingredient\n",
    "    # list not supported...\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "async def spaghetti():\n",
    "    '''lmql\n",
    "    \"Spaghetti bolognese recipe for a family of 4.\"\n",
    "    \"[RECIPE_DATA]\\\\n\" where type(RECIPE_DATA) is Recipe\n",
    "    return RECIPE_DATA\n",
    "    '''\n",
    "\n",
    "result = await spaghetti()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Recipe(recipe_name='Spaghetti Bolognese', servings=4, ingredient1=Ingredient(name='Spaghetti', weight_in_grams=454), ingredient2=Ingredient(name='Ground Beef', weight_in_grams=454), ingredient3=Ingredient(name='Olive Oil', weight_in_grams=2), ingredient4=Ingredient(name='Onion', weight_in_grams=150), ingredient5=Ingredient(name='Garlic', weight_in_grams=3), ingredient6=Ingredient(name='Tomato Sauce', weight_in_grams=845), ingredient7=Ingredient(name='Tomato Paste', weight_in_grams=113), ingredient8=Ingredient(name='Salt', weight_in_grams=2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_name': 'Spaghetti Bolognese',\n",
       " 'servings': 4,\n",
       " 'ingredient1': Ingredient(name='Spaghetti', weight_in_grams=450),\n",
       " 'ingredient2': Ingredient(name='Olive Oil', weight_in_grams=2),\n",
       " 'ingredient3': Ingredient(name='Onion', weight_in_grams=150),\n",
       " 'ingredient4': Ingredient(name='Garlic', weight_in_grams=2),\n",
       " 'ingredient5': Ingredient(name='Carrots', weight_in_grams=100),\n",
       " 'ingredient6': Ingredient(name='Celery', weight_in_grams=50),\n",
       " 'ingredient7': Ingredient(name='Ground Beef', weight_in_grams=400),\n",
       " 'ingredient8': Ingredient(name='Tomato Sauce', weight_in_grams=800)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ingredient(name='Spaghetti', weight_in_grams=450)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.ingredient1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ingredient1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Recipe.__dataclass_fields__['ingredient1'].name #Recipe.__dataclass_fields__ is a dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_name': str,\n",
       " 'servings': int,\n",
       " 'ingredient1': str,\n",
       " 'ingredient2': str,\n",
       " 'ingredient3': str}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Recipe.__annotations__ # dict with name:type "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own example with recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "recipe = \"\"\"Ingredients for 4 servings:\n",
    "• 1 cup all-purpose flour\n",
    "• 3 tablespoons granulated sugar\n",
    "• 1 teaspoon baking powder\n",
    "• 1/2 teaspoon baking soda\n",
    "• 1/2 teaspoon salt\n",
    "• 1 cup milk\n",
    "• 1 egg\n",
    "• 3 tablespoons unsalted butter, melted\n",
    "• Toppings of your choice (fresh fruit, whipped cream, syrup, etc.)\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Recipe:\n",
    "    recipe_name: str\n",
    "    servings: int\n",
    "    ingredient1: str\n",
    "    ingredient2: str\n",
    "    ingredient3: str\n",
    "\n",
    "@lmql.query(model=llm, verbose=False)\n",
    "async def get_recipe(recipe):\n",
    "    '''lmql\n",
    "    \"{recipe}\"\n",
    "    \"[RECIPE_DATA]\\\\n\" where type(RECIPE_DATA) is Recipe\n",
    "    return RECIPE_DATA\n",
    "    '''\n",
    "\n",
    "result = await get_recipe(recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_name': 'Buttermilk Pancakes',\n",
       " 'servings': 4,\n",
       " 'ingredient1': '1 cup all-purpose flour',\n",
       " 'ingredient2': '3 tablespoons granulated sugar',\n",
       " 'ingredient3': '1 teaspoon baking powder'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "recipe = \"\"\"Ingredients:\n",
    "• 1 cup all-purpose flour\n",
    "• 3 tablespoons granulated sugar\n",
    "• 1 teaspoon baking powder\n",
    "• 1/2 teaspoon baking soda\n",
    "• 1/2 teaspoon salt\n",
    "• 1 cup milk\n",
    "• 1 egg\n",
    "• 3 tablespoons unsalted butter, melted\n",
    "• Toppings of your choice (fresh fruit, whipped cream, syrup, etc.)\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Recipe:\n",
    "    recipe_name: str\n",
    "    servings: int\n",
    "    ingredient1: str\n",
    "    ingredient2: str\n",
    "    ingredient3: str\n",
    "\n",
    "field_descriptions = [\n",
    "    \"Generate a short recipe name\",\n",
    "    \"Generate number of servings based on ingredient amounts\",\n",
    "    \"Extract ingredient name only\",\n",
    "    \"Extract ingredient amount only\",\n",
    "    \"Extract ingredient name and ingredient amount\",\n",
    "]\n",
    "\n",
    "field_prompting = \"\"\n",
    "for field_name, field_descr in zip(Recipe.__annotations__.keys(), field_descriptions):\n",
    "    field_prompting += \"For field \" + \"'\" + field_name + \"'\" + \" \" + \"follow these instructions: \" + field_descr + \"\\n \"\n",
    "\n",
    "@lmql.query(model=llm, verbose=False)\n",
    "async def get_recipe(recipe, field_prompting):\n",
    "    '''lmql\n",
    "    \"{recipe}\"\n",
    "    \"{field_prompting}\"\n",
    "    \"[RECIPE_DATA]\\\\n\" where type(RECIPE_DATA) is Recipe\n",
    "    return RECIPE_DATA\n",
    "    '''\n",
    "\n",
    "result = await get_recipe(recipe, field_prompting)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# output result.__dict__:\n",
    "\n",
    "# {'recipe_name': 'Buttermilk Pancakes',\n",
    "# 'servings': 4,\n",
    "# 'ingredient1': 'all-purpose flour',\n",
    "# 'ingredient2': '3 tbsp',\n",
    "# 'ingredient3': '1 cup'}\n",
    "\n",
    "# -> ingredient3 should be name and amount\n",
    "# NOTE: bug with infinite List generation thus can not use ingredients: List(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recipe_name': 'Buttermilk Pancakes',\n",
       " 'servings': 4,\n",
       " 'ingredient1': 'all-purpose flour',\n",
       " 'ingredient2': '3 tbsp',\n",
       " 'ingredient3': '1 cup'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own example with article as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader \n",
    "  \n",
    "reader = PdfReader('/home/dorota/LLM-diploma-project/00_concept_tests/data/40001_2023_Article_1364.pdf') \n",
    "num_pages = len(reader.pages)\n",
    "TEXT = \"\"\n",
    "for page_num in range(1): #change to range(num_pages) for whole document\n",
    "    page = reader.pages[page_num]  \n",
    "    TEXT += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmql\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class NodeMetadata:\n",
    "    title: str\n",
    "    authors: str\n",
    "    pub_year: int\n",
    "    key_words: str\n",
    "    summary: str\n",
    "    research_area: str\n",
    "    quality: str\n",
    "    quality_reason: str\n",
    "\n",
    "field_descriptions = [\n",
    "    \"extract title from article\",\n",
    "    \"extract authors from article\",\n",
    "    \"extract publication year\",\n",
    "    \"generate 5 new key words based on content in Abstract\",\n",
    "    \"generate summary in 3 sentences\",\n",
    "    \"generate 1 main research area described in article\",\n",
    "    \"select 1 value from ['GOOD', 'BAD', 'EXCELLENT', 'CAN NOT SET QUALITY'] to define quality of article\",\n",
    "    \"describe reason for chosen quality_score in 1 sentece\",\n",
    "]\n",
    "\n",
    "field_prompting = \"\"\n",
    "for field_name, field_descr in zip(NodeMetadata.__annotations__.keys(), field_descriptions):\n",
    "    field_prompting += \"For field \" + \"'\" + field_name + \"'\" + \" \" + \"follow these instructions: \" + field_descr + \"\\n \"\n",
    "\n",
    "@lmql.query(model=llm, verbose=False)\n",
    "async def get_recipe(TEXT, field_prompting):\n",
    "    '''lmql\n",
    "    \"{TEXT}\"\n",
    "    \"{field_prompting}\"\n",
    "    \"[NODE_METADATA]\\\\n\" where type(NODE_METADATA) is NodeMetadata\n",
    "    return NODE_METADATA\n",
    "    '''\n",
    "\n",
    "result = await get_recipe(TEXT, field_prompting)\n",
    "result.__dict__\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "# output result.__dict__:\n",
    "#{'title': 'Visualization of breast cancer -related protein synthesis from the perspective of bibliometric analysis',\n",
    "# 'authors': 'Jiawei Xu, Chengdong Yu, Xiaoqiang Zeng, Weifeng Tang, Siyi Xu, Lei Tang, Yanxiao Huang, Zhengkui Sun, Tenghua Yu',\n",
    "# 'pub_year': 202,\n",
    "# 'key_words': 'Breast cancer, Bibliometric analysis, Protein synthesis, Expression, Cancer, Protein, Translation',\n",
    "# 'summary': \"This article provides insights into the research on breast cancer and protein synthesis through bibliometric analysis. The analysis reveals a steady increase in publications, with most articles published in oncology or biology-related journals. Keyword analysis shows that 'breast cancer,' 'expression,' 'cancer,' 'protein,' and 'translation' are the most commonly researched topics. The research focuses on the relationship between protein expression in breast cancer and tumor development and treatment.\",\n",
    "# 'research_area': 'Breast cancer research, Protein synthesis research',\n",
    "# 'quality': 'GOOD',\n",
    "# 'quality_reason': 'The article provides a comprehensive analysis of the literature on breast cancer and protein synthesis, using reliable data sources and valid analytical methods.'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: generate field_prompting from Pydantic dictionary and tranform reulting result.__dict__ to Pydantic\n",
    "#TODO: create a loop to include SQL restrictions\n",
    "#TODO: kör med olika noder via VectorStore Semantic uppdelning på noder\n",
    "#TODO: kan inte köra hela artikeln även om n_ctx=0 så testa att utöka max_content (eller vad den nu klagade på) eller kör med olika noder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Chain of thought example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The day one week ago was the 5th of June.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "@lmql.query(model=llm)\n",
    "def chain_of_thought(question):\n",
    "    '''lmql\n",
    "    # Q&A prompt template\n",
    "    \"Q: {question}\\n\"\n",
    "    \"A: Let's think step by step.\\n\"\n",
    "    \"[REASONING]\"\n",
    "    \"Thus, the answer is:[ANSWER].\" where STOPS_AT(ANSWER, \".\")\n",
    "\n",
    "    # return just the ANSWER to the caller\n",
    "    return ANSWER.strip()\n",
    "    '''\n",
    "\n",
    "result = chain_of_thought('Today is the 12th of June, what day was it 1 week ago?')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data with SimpleDirectoryReader\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/\n",
    "\n",
    "more readers availble at https://llamahub.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You can specify a function that will read each file and extract metadata that gets attached to the resulting Document\n",
    "# def get_meta(file_path):\n",
    "#     return {\"foo\": \"bar\", \"file_path\": file_path}\n",
    "\n",
    "\n",
    "# SimpleDirectoryReader(input_files=[\"/home/dorota/LLM-diploma-project/00_concept_tests/data/40001_2023_Article_1364.pdf\"], file_metadata=get_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # additional possibilities with SimpleDirectoryReader\n",
    "# documents = SimpleDirectoryReader(input_dir=\"/home/dorota/LLM-diploma-project/00_concept_tests/data\", recursive=True).load_data(num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create nodes with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. SentenceSplitter\n",
    "The SentenceSplitter attempts to split text in chunks while respecting the boundaries of sentences. \\\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # can be defined globaly\n",
    "# Settings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "# # an be dafound per-index through transformations\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SentenceWindowNodeParser\n",
    "Splits all documents into individual sentences. The resulting nodes also contain the surrounding \"window\" of sentences around each node in the metadata.\\\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=2,  # how many sentences on either side to capture\n",
    "    window_metadata_key=\"window\", # the metadata key that holds the window of surrounding sentences\n",
    "    original_text_metadata_key=\"original_sentence\", # the metadata key that holds the original sentence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[3].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SemanticSplitterNodeParser\n",
    "https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "\n",
    "splitter = SemanticSplitterNodeParser(\n",
    "    buffer_size=1, breakpoint_percentile_threshold=95, embed_model=Settings.embed_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nodes[2].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. HierarchicalNodeParser\n",
    "Input is chunked into several hierarchies of chunk sizes, with each node containing a reference to it's parent node. https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/ \\\n",
    "When combined with the AutoMergingRetriever, this enables us to automatically replace retrieved nodes with their parents when a majority of children are retrieved. https://docs.llamaindex.ai/en/stable/examples/retrievers/auto_merging_retriever/ (conclusion in tutorial that output quality similar to non hierarchical approach...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk into parent, child, grandchild (leaf) nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "\n",
    "splitter = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128] # chunk size parent, child, grandchild\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolate grandchild nodes from root nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import get_leaf_nodes, get_root_nodes\n",
    "\n",
    "base_nodes = get_leaf_nodes(nodes)\n",
    "root_nodes = get_root_nodes(nodes)\n",
    "\n",
    "len(base_nodes), len(root_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all nodes into SimpleDocumentStore and only leaf nodes into VectoreStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore) # define storage context (will include vector store by default too)\n",
    "\n",
    "## Load index into vector index\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "base_index = VectorStoreIndex(\n",
    "    base_nodes,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=3)\n",
    "retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n",
    "\n",
    "# query_str = (\"What is the title of the article?\")\n",
    "query_str = (\"What is the main topic of the article?\")\n",
    "\n",
    "nodes = retriever.retrieve(query_str)\n",
    "base_nodes = base_retriever.retrieve(query_str)\n",
    "\n",
    "len(nodes), len(base_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "import matplotlib\n",
    "\n",
    "for node in base_nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes:\n",
    "    display_source_node(node, source_length=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TokenTextSplitter https://docs.llamaindex.ai/en/stable/module_guides/loading/documents_and_nodes/usage_metadata_extractor/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: seem to be the same output: nodes.get_content(), nodes.text, nodes.get_text()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
