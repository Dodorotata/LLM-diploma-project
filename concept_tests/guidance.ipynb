{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of {Guidance}:\n",
    "* Llama-index guidance pydantic program\n",
    "* Microsoft guidance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "CONCLUSIONS:\n",
    "* Llama-index guidance pydantic program dicarded due to LLM not outputting correct json schema even with mixtral. # -> OutputParserException: Failed to parse pydantic object from guidance program. Probably the LLM failed to produce data with right json schema\n",
    "* Native guidance with mistral_Q6 unstable output especially with lists - probably due to quantatization of model.\n",
    "* Extract data to json works well for select from a list of choices and generation\n",
    "* Content of the generated output from longer text or scientific article varies. Output an be improved with prompting: propmpt engineering, chain-of-thought, few-shot examples\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Llama-index guidance pydantic program\n",
    "https://docs.llamaindex.ai/en/stable/examples/output_parsing/guidance_pydantic_program/?h=guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "from llama_index.program.guidance import GuidancePydanticProgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "mixtral = guidance.models.LlamaCppChat(\"/home/dorota/models/mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf\") # https://guidance.readthedocs.io/en/latest/api.html\n",
    "# mistral_Q4 = guidance.models.LlamaCppChat(\"/home/dorota/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\") # https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf?download=true\n",
    "# mistral_Q6 = guidance.models.LlamaCppChat(\"/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\") # https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q6_K.gguf?download=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Book(BaseModel):\n",
    "    autor: str\n",
    "    title: str\n",
    "    pub_year: int\n",
    "\n",
    "\n",
    "class BookShelf(BaseModel):\n",
    "    genre: str\n",
    "    books: List[Book]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "program = GuidancePydanticProgram(\n",
    "    output_cls=BookShelf,\n",
    "    prompt_template_str=(\n",
    "        \"Generate a bookshelf with 3 books within a genre with an author, a title and a publication year. Using\"\n",
    "        \" genre '{{query_str}}' as inspiration. Parse output as json\"\n",
    "    ),\n",
    "    guidance_llm=mixtral,\n",
    "    verbose=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = program(query_str=\"Science Fiction\", tools_str='')\n",
    "\n",
    "# -> OutputParserException: Failed to parse pydantic object from guidance program. Probably the LLM failed to produce data with right json schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## {Guidance}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/guidance-ai/guidance/blob/main/notebooks/tutorials/intro_to_guidance.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "from guidance import models, gen\n",
    "\n",
    "# mistral = guidance.models.LlamaCpp(\"/home/dorota/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\", n_gpu_layers=-1, n_ctx=4096)\n",
    "mistral = guidance.models.LlamaCpp(\"/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = mistral + \"Who won the last Kentucky derby and by how much?\"\n",
    "lm + gen(max_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral + '''\\\n",
    "Q: Who won the last Kentucky derby and by how much?\n",
    "A:''' + gen(stop=\"Q:\")\n",
    "\n",
    "# -> expected answer most of the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using f strings and naming elements of the output\n",
    "\n",
    "query = \"Who won the last Kentucky derby and by how much?\"\n",
    "lm = mistral + f'''\\\n",
    "Q: {query}\n",
    "A: {gen(name='answer', stop=\"Q:\")}'''\n",
    "\n",
    "# -> expected answer\n",
    "\n",
    "lm['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function encapsulation\n",
    "\n",
    "@guidance\n",
    "def qa_bot(lm, query):\n",
    "    lm = lm + f'''\\\n",
    "    Q: {query}\n",
    "    A: {gen(name=\"answer\", stop=\"Q:\")}'''\n",
    "    return lm\n",
    "\n",
    "query = \"Who won the last Kentucky derby and by how much?\"\n",
    "mistral + qa_bot(query) # note we don't pass the `lm` arg here (that will get passed during execution when it gets added to the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting among alternatives\n",
    "\n",
    "from guidance import select\n",
    "\n",
    "query = \"Who won the last Kentucky derby and by how much?\"\n",
    "\n",
    "mistral + f'''\\\n",
    "Q: {query}\n",
    "Now I will choose to either SEARCH the web or RESPOND.\n",
    "Choice: {select([\"SEARCH\", \"RESPOND\"], name=\"choice\")}''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@guidance\n",
    "def qa_bot(lm, query):\n",
    "    lm += f'''\\\n",
    "    Q: {query}\n",
    "    Now I will choose to either SEARCH the web or RESPOND.\n",
    "    Choice: {select([\"SEARCH\", \"RESPOND\"], name=\"choice\")}\n",
    "    '''\n",
    "    if lm[\"choice\"] == \"SEARCH\":\n",
    "        lm += \"A: I don't know, Google it!\"\n",
    "    else:\n",
    "        lm += f'A: {gen(stop=\"Q:\", name=\"answer\")}'\n",
    "    return lm\n",
    "\n",
    "query = \"Who won the last Kentucky derby and by how much?\"\n",
    "mistral + qa_bot(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating lists\n",
    "\n",
    "query = \"Who won the last Kentucky derby and by how much?\"\n",
    "\n",
    "lm = mistral + f'''\\\n",
    "Q: {query}\n",
    "Now I will choose to either SEARCH the web or RESPOND.\n",
    "Choice: {select([\"SEARCH\", \"RESPOND\"], name=\"choice\")}\n",
    "'''\n",
    "if lm[\"choice\"] == \"SEARCH\":\n",
    "    lm += \"Here are 3 search queries:\\n\"\n",
    "    for i in range(3):\n",
    "        lm += f'''{i+1}. \"{gen(stop='\"', name=\"queries\", temperature=1.0, list_append=True)}\"\\n'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/guidance-ai/guidance?tab=readme-ov-file#guidance-acceleration\n",
    "\n",
    "from guidance import substring\n",
    "\n",
    "# define a set of possible statements\n",
    "text = 'guidance is awesome. guidance is so great. guidance is the best thing since sliced bread.'\n",
    "\n",
    "# force the model to make an exact quote\n",
    "mistral + f'Here is a true statement about the guidance library: \"{substring(text)}\"'\n",
    "\n",
    "# -> not substring is selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Guidance -> json\n",
    "https://github.com/guidance-ai/guidance/blob/main/notebooks/guaranteeing_valid_syntax.ipynb \\\n",
    "https://github.com/guidance-ai/guidance?tab=readme-ov-file#guidance-acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "from guidance import models, gen, select\n",
    "\n",
    "lm = guidance.models.LlamaCpp(\"/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", n_gpu_layers=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can pre-define valid option sets\n",
    "sample_weapons = [\"sword\", \"axe\", \"mace\", \"spear\", \"bow\", \"crossbow\"]\n",
    "sample_armor = [\"leather\", \"chainmail\", \"plate\"]\n",
    "\n",
    "# define a re-usable \"guidance function\" that we can use below\n",
    "@guidance\n",
    "def quoted_list(lm, name, n):\n",
    "    for i in range(n):\n",
    "        if i > 0:\n",
    "            lm += \", \"\n",
    "        lm += '\"' + gen(name, list_append=True, stop='\"') + '\"'\n",
    "    return lm\n",
    "\n",
    "@guidance\n",
    "def generate_character(\n",
    "    lm,\n",
    "    character_one_liner,\n",
    "    weapons: list[str] = sample_weapons,\n",
    "    armour: list[str] = sample_armor,\n",
    "    n_items: int = 3\n",
    "):\n",
    "    lm += f'''\\\n",
    "    {{\n",
    "        \"description\" : \"{character_one_liner}\",\n",
    "        \"name\" : \"{gen(\"character_name\", stop='\"')}\",\n",
    "        \"age\" : {gen(\"age\", regex=\"[0-9]+\")},\n",
    "        \"armour\" : \"{select(armour, name=\"armor\")}\",\n",
    "        \"weapon\" : \"{select(weapons, name=\"weapon\")}\",\n",
    "        \"class\" : \"{gen(\"character_class\", stop='\"')}\",\n",
    "        \"mantra\" : \"{gen(\"mantra\", stop='\"')}\",\n",
    "        \"strength\" : {gen(\"age\", regex=\"[0-9]+\")},\n",
    "        \"quest_items\" : [{quoted_list(\"quest_items\", n_items)}]\n",
    "    }}'''\n",
    "    return lm\n",
    "\n",
    "\n",
    "generation = lm + generate_character(\"A quick and nimble fighter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "gen_json = json.loads(str(generation))\n",
    "print(f\"Loaded json:\\n{json.dumps(gen_json, indent=4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(generation), type(gen_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation['weapon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## POC extract data -> json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import guidance\n",
    "from guidance import models, gen, select\n",
    "import json\n",
    "\n",
    "lm = guidance.models.LlamaCpp(\"/home/dorota/models/mistral-7b-instruct-v0.2.Q6_K.gguf\", n_gpu_layers=-1, n_ctx=0, echo=False)\n",
    "\n",
    "# n_ctx=0 is 16384 (max); default n_ctx=512; echo=False -> query and generation output not displayed; n_gpu_layers=0 CPU only, =-1 GPU only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recepie = \"\"\"Ingredients:\n",
    "• 1 cup all-purpose flozfgvzdfgur\n",
    "• 3 tablespoons granulated sugar\n",
    "• 1 teaspoon baking powder\n",
    "• 1/2 teaspoon baking soda\n",
    "• 1/2 teaspoon salt\n",
    "• 1 cup milk\n",
    "• 1 egg\n",
    "• 3 tablespoons unsalted butter, melted\n",
    "• Toppings of your choice (fresh fruit, whipped cream, syrup, etc.)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intial prompt\n",
    "\n",
    "@guidance\n",
    "def extract_alergens(lm,recepie):\n",
    "    lm += f'''\\\n",
    "    Indicate if the recepie contains alergens. Recepie is deliminated by (start) and (stop). \n",
    "    Output 'yes' if alergen can be found or 'no' if algergen can not be found. Generated output should be a string.\n",
    "    --------------------------------------(start)\n",
    "    {recepie}\n",
    "    (stop) -------------------------------------- \n",
    "    {{\n",
    "        \"eggs\" : \"{gen(\"eggs\", stop='\"')}\",\n",
    "        \"milk\" : \"{gen(\"milk\", stop='\"')}\",\n",
    "        \"flour\" : \"{gen(\"flour\", stop='\"')}\",\n",
    "        \"soy\" : \"{gen(\"soy\", stop='\"')}\",\n",
    "    }}'''\n",
    "    return lm\n",
    "\n",
    "\n",
    "generation = lm + extract_alergens(recepie)\n",
    "\n",
    "# 1 cup all-purpose flozfgvzdfgur is still interpreted as flour\n",
    "# 1 cup of cream is still interpreted as milk but 1 cookie is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt with few-shot example\n",
    "@guidance\n",
    "def extract_alergens(lm,recepie):\n",
    "    lm += f'''\\\n",
    "    Indicate if the recepie contains ingredients. Recepie is deliminated by (start) and (stop). \n",
    "    Output 'yes' if ingredient can be found in recepie or 'no' if ingredient can not be found in recepie.\n",
    "    This is example output:\n",
    "     ```json\n",
    "    {{\n",
    "        \"egg\" : \"yes\",\n",
    "        \"milk\" : \"yes\",\n",
    "        \"sugar\" : \"yes\",\n",
    "        \"sand\" : \"no\"\n",
    "    }}``\n",
    "    --------------------------------------(start)\n",
    "    {recepie}\n",
    "    (stop) -------------------------------------- \n",
    "    ```json\n",
    "    {{\n",
    "        \"egg\" : \"{gen(\"eggs\", stop='\"')}\",\n",
    "        \"milk\" : \"{gen(\"milk\", stop='\"')}\",\n",
    "        \"flour\" : \"{gen(\"flour\", stop='\"')}\",\n",
    "        \"soy\" : \"{gen(\"soy\", stop='\"')}\"\n",
    "    }}```'''\n",
    "    return lm\n",
    "\n",
    "\n",
    "generation = lm + extract_alergens(recepie)\n",
    "\n",
    "# -> correct output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@guidance\n",
    "def extract_alergens(lm, recepie):\n",
    "    lm += f'''\\\n",
    "    Indicate if the recepie contains alergens. Recepie is deliminated by (start) and (stop).\n",
    "    --------------------------------------(start)\n",
    "    {recepie}\n",
    "    (stop)--------------------------------------\n",
    "    {{\n",
    "        \"egg\" : \"{select(options=['yes', 'no'], name='egg')}\",\n",
    "        \"milk\" : \"{select(options=['yes', 'no'], name='milk')}\",\n",
    "        \"flour\" : \"{select(options=['yes', 'no'], name='flour')}\",\n",
    "        \"gluten\" : \"{select(options=['yes', 'no'], name='gluten')}\",\n",
    "        \"soy\" : \"{select(options=['yes', 'no'], name='soy')}\",\n",
    "        \"egg_amount\" : {gen(\"egg_amount\", regex=\"[0-9]+\")}\n",
    "        \"soy_amount\" : {gen(\"soy_amount\", regex=\"[0-9]+\")}\n",
    "    }}'''\n",
    "    return lm\n",
    "\n",
    "generation = lm + extract_alergens(recepie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one shot prompting\n",
    "@guidance\n",
    "def extract_alergens(lm, recepie):\n",
    "    lm += f'''\\\n",
    "    Indicate if the recepie contains alergens. Alergen can for example be egg, milk. Recepie is deliminated by (start) and (stop). Only select 'yes' if the alergen is explicitly mentioned in the recepie.\n",
    "    --------------------------------------(start)\n",
    "    {recepie}\n",
    "    (stop)--------------------------------------\n",
    "    generated_output:\n",
    "    {{\n",
    "        \"egg\" : \"{select(options=['yes', 'no'], name='egg')}\",\n",
    "        \"milk\" : \"{select(options=['yes', 'no'], name='milk')}\",\n",
    "        \"flour\" : \"{select(options=['yes', 'no'], name='flour')}\",\n",
    "        \"gluten\" : \"{select(options=['yes', 'no'], name='gluten')}\",\n",
    "        \"soy\" : \"{select(options=['yes', 'no'], name='soy')}\",\n",
    "        \"egg_amount\" : {gen(\"egg_amount\", regex=\"[0-9]+\")}\n",
    "        \"soy_amount\" : {gen(\"soy_amount\", regex=\"[0-9]+\")}\n",
    "    }}\n",
    "    \n",
    "    Check if generated output is correct. \n",
    "    correctd_generated_output:\n",
    "    {{\n",
    "        \"egg\" : \"{select(options=['yes', 'no'], name='egg')}\",\n",
    "        \"milk\" : \"{select(options=['yes', 'no'], name='milk')}\",\n",
    "        \"flour\" : \"{select(options=['yes', 'no'], name='flour')}\",\n",
    "        \"gluten\" : \"{select(options=['yes', 'no'], name='gluten')}\",\n",
    "        \"soy\" : \"{select(options=['yes', 'no'], name='soy')}\",\n",
    "        \"egg_amount\" : {gen(\"egg_amount\", regex=\"[0-9]+\")}\n",
    "        \"soy_amount\" : {gen(\"soy_amount\", regex=\"[0-9]+\")}\n",
    "    }}'''\n",
    "    return lm\n",
    "\n",
    "generation = lm + extract_alergens(recepie)\n",
    "\n",
    "# correct and consistent output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTICLE = \"\"\" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.\n",
    "A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.\n",
    "Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared \"I do\" five more times, sometimes only within two weeks of each other.\n",
    "In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her \"first and only\" marriage.\n",
    "Barrientos, now 39, is facing two criminal counts of \"offering a false instrument for filing in the first degree,\" referring to her false statements on the\n",
    "2010 marriage license application, according to court documents.\n",
    "Prosecutors said the marriages were part of an immigration scam.\n",
    "On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.\n",
    "After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective\n",
    "Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.\n",
    "All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.\n",
    "Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.\n",
    "Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.\n",
    "Seven of the men are from so-called \"red-flagged\" countries, including Egypt, Turkey, Georgia, Pakistan and Mali.\n",
    "Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.\n",
    "If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.\n",
    "\"\"\"\n",
    "\n",
    "@guidance\n",
    "def summarize(lm, text):\n",
    "    lm += f'''\\\n",
    "    Generate a title and summary for the text. Text is deliminated by (start) and (stop). Generated title and summary must be strings.\n",
    "    ---------------------------------------------------------(start)\n",
    "    {text}\n",
    "    (stop)----------------------------------------------------------\n",
    "\n",
    "    structured_output:\n",
    "    {{\n",
    "        \"text_title\" : \"{gen(name=\"title\", stop='\"')}\",\n",
    "        \"text_summary\" : \"{gen(name=\"summary\", stop='\"')}\"\n",
    "    }}'''\n",
    "    return lm\n",
    "\n",
    "generation = lm + summarize(ARTICLE)\n",
    "\n",
    "print(f'''\\\n",
    "      {{\n",
    "        \"title\" : \"{generation[\"title\"]}\",\n",
    "        \"summary\" : \"{generation[\"summary\"]}\",\n",
    "      }}\n",
    "''')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write structured output part of response to json string and further to dict\n",
    "import json\n",
    "\n",
    "gen_json_string = str(generation).split('structured_output:\\n')[-1]\n",
    "gen_dict = json.loads(gen_json_string)\n",
    "print(json.dumps(gen_dict, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdf article as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader \n",
    "  \n",
    "reader = PdfReader('/home/dorota/LLM-diploma-project/00_concept_tests/data/40001_2023_Article_1364.pdf') \n",
    "num_pages = len(reader.pages)\n",
    "TEXT = \"\"\n",
    "for page_num in range(1): #change to range(num_pages) for whole document\n",
    "    page = reader.pages[page_num]  \n",
    "    TEXT += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@guidance\n",
    "def get_metadata(lm, text):\n",
    "    lm += f'''\\\n",
    "    Article is delimited by (start) and (stop). Extract title, authors and publication year. Generate 5 keywords based on Abstract, do not use keywords in article. Summarize Abstract. Generate research area.\n",
    "\n",
    "    (start)\n",
    "    {text}\n",
    "    (stop)\n",
    "\n",
    "    JSON ouput:\n",
    "    {{\n",
    "        \"title\" : \"{gen(name=\"title\", stop='\"')}\",\n",
    "        \"authors\" : \"{gen(name=\"authors\", stop='\"')}\",\n",
    "        \"publication_year\": {gen(\"publication_year\", regex=\"[0-9]+\")},\n",
    "        \"key_words\" : \"{gen(name=\"key_words\", stop='\"')}\",\n",
    "        \"summary\" : \"{gen(name=\"summary\", stop='\"')}\",\n",
    "        \"research_area\" : \"{gen(name=\"research_area\", stop='\"')}\",\n",
    "        \"quality_score\" : \"{select(options=['GOOD', 'BAD', 'EXCELLENT'], name=\"quality\")}\",\n",
    "        \"quality_reason\" : \"{gen(name=\"quality_reason\", stop='\"')}\"\n",
    "    }}'''\n",
    "    return lm\n",
    "\n",
    "generation = lm + get_metadata(TEXT)\n",
    "\n",
    "print(f'''\\\n",
    "      {{\n",
    "        \"title\" : \"{generation[\"title\"]}\",\n",
    "        \"authors\" : \"{generation[\"authors\"]}\",\n",
    "        \"publication_year\": \"{generation[\"publication_year\"]}\",\n",
    "        \"key_words\" : \"{generation[\"key_words\"]}\",\n",
    "        \"summary\" : \"{generation[\"summary\"]}\",\n",
    "        \"research_area\" : \"{generation[\"research_area\"]}\",\n",
    "        \"quality_score\" : \"{generation[\"quality\"]}\",\n",
    "        \"quality_reason\" : \"{generation[\"quality_reason\"]}\"\n",
    "      }}\n",
    "''')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
